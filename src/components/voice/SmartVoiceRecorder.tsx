import React, { useState, useRef, useEffect, useCallback } from 'react';
import { MicVAD } from '@ricky0123/vad-web';
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import { Progress } from '@/components/ui/progress';
import { Slider } from '@/components/ui/slider';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Separator } from '@/components/ui/separator';
import { cn } from '@/lib/utils';
import { Mic, Square, Settings, Volume2, Activity, Shield } from 'lucide-react';

// å½•éŸ³çŠ¶æ€æšä¸¾
enum RecordingState {
  IDLE = 'idle',
  LISTENING = 'listening', 
  RECORDING = 'recording',
  PROCESSING = 'processing',
  ERROR = 'error'
}

// VADé…ç½®æ¥å£
interface VADConfig {
  positiveSpeechThreshold: number;
  negativeSpeechThreshold: number;
  minSpeechFrames: number;
  preSpeechPadFrames: number;
  redemptionFrames: number;
}

// éŸ³é¢‘çŠ¶æ€æ¥å£
interface AudioMetrics {
  volume: number;
  frequency: number;
  speechConfidence: number;
  noiseLevel: number;
}

// å½•éŸ³é…ç½®æ¥å£
interface RecordingConfig {
  silenceTimeout: number; // é™éŸ³è¶…æ—¶ï¼ˆç§’ï¼‰
  maxRecordingTime: number; // æœ€å¤§å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰
  preRecordingBuffer: number; // é¢„å½•åˆ¶ç¼“å†²ï¼ˆç§’ï¼‰
  sampleRate: number;
  enableNoiseReduction: boolean;
}

// æ™ºèƒ½é™å™ªé…ç½®æ¥å£ - æ–°å¢
interface SmartNoiseConfig {
  enableVolumeGating: boolean; // éŸ³é‡é—¨æ§
  volumeThreshold: number; // éŸ³é‡é˜ˆå€¼
  enableFrequencyFiltering: boolean; // é¢‘ç‡æ»¤æ³¢
  enableDirectionalDetection: boolean; // æ–¹å‘æ€§æ£€æµ‹
  enableAdaptiveThreshold: boolean; // è‡ªé€‚åº”é˜ˆå€¼
  backgroundNoiseLevel: number; // èƒŒæ™¯å™ªéŸ³åŸºå‡†
}

// ç»„ä»¶Propsæ¥å£
interface SmartVoiceRecorderProps {
  onAudioReady: (audioBlob: Blob, metrics: AudioMetrics) => void;
  onStateChange?: (state: RecordingState) => void;
  isLoading?: boolean;
  isPaused?: boolean;
  showAdvancedControls?: boolean;
  className?: string;
  // æ–°å¢ï¼šå¤–éƒ¨é…ç½®ä¼ å…¥
  externalVadConfig?: Partial<VADConfig>;
  externalSmartNoiseConfig?: Partial<SmartNoiseConfig>;
  externalRecordingConfig?: Partial<RecordingConfig>;
}

const SmartVoiceRecorder: React.FC<SmartVoiceRecorderProps> = ({
  onAudioReady,
  onStateChange,
  isLoading = false,
  isPaused = false,
  showAdvancedControls = false,
  className,
  externalVadConfig,
  externalSmartNoiseConfig,
  externalRecordingConfig
}) => {
  // åŸºç¡€çŠ¶æ€
  const [recordingState, setRecordingState] = useState<RecordingState>(RecordingState.IDLE);
  const [micStream, setMicStream] = useState<MediaStream | null>(null);
  const [error, setError] = useState<string | null>(null);

  // VADç›¸å…³
  const vadRef = useRef<MicVAD | null>(null);
  const [isVadReady, setIsVadReady] = useState(false);
  const [vadConfig, setVadConfig] = useState<VADConfig>({
    positiveSpeechThreshold: 0.6,
    negativeSpeechThreshold: 0.4,
    minSpeechFrames: 3,
    preSpeechPadFrames: 2,
    redemptionFrames: 10,
  });

  // éŸ³é¢‘åˆ†æ
  const [audioMetrics, setAudioMetrics] = useState<AudioMetrics>({
    volume: 0,
    frequency: 0,
    speechConfidence: 0,
    noiseLevel: 0,
  });

  // å½•éŸ³ç›¸å…³
  const [recordingConfig, setRecordingConfig] = useState<RecordingConfig>({
    silenceTimeout: 2,
    maxRecordingTime: 30,
    preRecordingBuffer: 1,
    sampleRate: 16000,
    enableNoiseReduction: true,
  });

  // æ™ºèƒ½é™å™ªé…ç½®
  const [smartNoiseConfig, setSmartNoiseConfig] = useState<SmartNoiseConfig>({
    enableVolumeGating: true,
    volumeThreshold: 15,
    enableFrequencyFiltering: true,
    enableDirectionalDetection: true,
    enableAdaptiveThreshold: true,
    backgroundNoiseLevel: 0,
  });

  // æ–°å¢çŠ¶æ€ï¼šè¯­éŸ³æ£€æµ‹çŠ¶æ€
  const [isSpeechDetected, setIsSpeechDetected] = useState(false);
  const [lastSpeechTime, setLastSpeechTime] = useState<number>(0);

  // å½•éŸ³æ•°æ®ç®¡ç†
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const recordingChunks = useRef<BlobPart[]>([]);
  const preRecordingBuffer = useRef<Float32Array[]>([]);
  
  // å®šæ—¶å™¨
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null);
  const maxRecordingTimerRef = useRef<NodeJS.Timeout | null>(null);
  const animationFrameRef = useRef<number>(0);

  // ç»Ÿè®¡æ•°æ®
  const [recordingStartTime, setRecordingStartTime] = useState<number>(0);
  const [speechStartTime, setSpeechStartTime] = useState<number>(0);

  // æ™ºèƒ½é™å™ªç›¸å…³çŠ¶æ€ - æ–°å¢
  const backgroundNoiseSamples = useRef<number[]>([]);
  const volumeHistory = useRef<number[]>([]);
  const speechQualityHistory = useRef<number[]>([]);
  const [isBackgroundCalibrated, setIsBackgroundCalibrated] = useState(false);

  // æ›´æ–°å½•éŸ³çŠ¶æ€
  const updateRecordingState = useCallback((newState: RecordingState) => {
    setRecordingState(newState);
    onStateChange?.(newState);
  }, [onStateChange]);

  // èƒŒæ™¯å™ªéŸ³æ ¡å‡† - æ–°å¢
  const calibrateBackgroundNoise = useCallback(() => {
    if (backgroundNoiseSamples.current.length > 50) {
      const avgNoise = backgroundNoiseSamples.current.reduce((a, b) => a + b, 0) / backgroundNoiseSamples.current.length;
      setSmartNoiseConfig(prev => ({
        ...prev,
        backgroundNoiseLevel: avgNoise * 1.2 // ç¨å¾®é«˜äºå¹³å‡å€¼
      }));
      setIsBackgroundCalibrated(true);
      console.log('èƒŒæ™¯å™ªéŸ³æ ¡å‡†å®Œæˆ:', avgNoise.toFixed(2));
    }
  }, []);

  // å“åº”å¤–éƒ¨é…ç½®å˜åŒ–
  useEffect(() => {
    if (externalVadConfig) {
      setVadConfig(prev => ({ ...prev, ...externalVadConfig }));
    }
  }, [externalVadConfig]);

  useEffect(() => {
    if (externalSmartNoiseConfig) {
      setSmartNoiseConfig(prev => ({ ...prev, ...externalSmartNoiseConfig }));
    }
  }, [externalSmartNoiseConfig]);

  useEffect(() => {
    if (externalRecordingConfig) {
      setRecordingConfig(prev => ({ ...prev, ...externalRecordingConfig }));
    }
  }, [externalRecordingConfig]);

  // æ™ºèƒ½éŸ³é¢‘åˆ†æ - å¢å¼ºç‰ˆï¼Œè¿”å›æ›´è¯¦ç»†çš„ä¿¡æ¯
  const analyzeAudioIntelligently = useCallback((volume: number, frequency: number, speechConfidence: number, noiseLevel: number): { 
    isRealSpeech: boolean; 
    reason: string;
    confidence: number;
  } => {
    let confidence = 0;
    let reasons: string[] = [];

    console.log(`[æ™ºèƒ½åˆ†æ] éŸ³é‡:${volume.toFixed(1)}% é¢‘ç‡:${frequency.toFixed(0)}Hz è¯­éŸ³ç½®ä¿¡åº¦:${speechConfidence.toFixed(1)}% å™ªéŸ³:${noiseLevel.toFixed(1)}%`);

    // 1. éŸ³é‡é—¨æ§æ£€æŸ¥
    if (smartNoiseConfig.enableVolumeGating && volume < smartNoiseConfig.volumeThreshold) {
      return { isRealSpeech: false, reason: `éŸ³é‡è¿‡ä½(${volume.toFixed(1)}% < ${smartNoiseConfig.volumeThreshold}%)`, confidence: 0 };
    }
    confidence += 20;
    reasons.push('éŸ³é‡åˆæ ¼');

    // 2. èƒŒæ™¯å™ªéŸ³è‡ªé€‚åº”
    if (smartNoiseConfig.enableAdaptiveThreshold) {
      // æ”¶é›†èƒŒæ™¯å™ªéŸ³æ ·æœ¬
      if (recordingState === RecordingState.LISTENING && volume < 10) {
        backgroundNoiseSamples.current.push(noiseLevel);
        if (backgroundNoiseSamples.current.length > 100) {
          backgroundNoiseSamples.current.shift();
        }
        
        if (!isBackgroundCalibrated && backgroundNoiseSamples.current.length >= 50) {
          calibrateBackgroundNoise();
        }
      }

      if (isBackgroundCalibrated && noiseLevel > smartNoiseConfig.backgroundNoiseLevel * 0.8) {
        return { isRealSpeech: false, reason: `èƒŒæ™¯å™ªéŸ³è¿‡é«˜(${noiseLevel.toFixed(1)}% > ${(smartNoiseConfig.backgroundNoiseLevel * 0.8).toFixed(1)}%)`, confidence: 0 };
      }
      confidence += 20;
      reasons.push('å™ªéŸ³æ°´å¹³æ­£å¸¸');
    }

    // 3. é¢‘ç‡ç‰¹å¾åˆ†æ
    if (smartNoiseConfig.enableFrequencyFiltering) {
      const isInHumanVoiceRange = frequency >= 800 && frequency <= 2000;
      if (!isInHumanVoiceRange && speechConfidence < 70) {
        return { isRealSpeech: false, reason: `é¢‘ç‡ä¸åœ¨äººå£°èŒƒå›´(${frequency.toFixed(0)}Hz ä¸åœ¨800-2000Hz)ä¸”ç½®ä¿¡åº¦ä½`, confidence: 0 };
      }
      confidence += 20;
      reasons.push('é¢‘ç‡åŒ¹é…');
    }

    // 4. æ–¹å‘æ€§æ£€æµ‹
    if (smartNoiseConfig.enableDirectionalDetection) {
      volumeHistory.current.push(volume);
      if (volumeHistory.current.length > 10) {
        volumeHistory.current.shift();
      }

      if (volumeHistory.current.length >= 5) {
        const recentVolumes = volumeHistory.current.slice(-5);
        const avgVolume = recentVolumes.reduce((a, b) => a + b) / recentVolumes.length;
        const volumeVariance = recentVolumes.reduce((acc, vol) => {
          return acc + Math.pow(vol - avgVolume, 2);
        }, 0) / recentVolumes.length;

        if (volumeVariance < 2 && volume > 10) {
          return { isRealSpeech: false, reason: `éŸ³é‡å˜åŒ–å¤ªå°(æ–¹å·®:${volumeVariance.toFixed(1)})ï¼Œç–‘ä¼¼èƒŒæ™¯å£°éŸ³`, confidence: 0 };
        }
        confidence += 20;
        reasons.push('éŸ³é‡å˜åŒ–æ­£å¸¸');
      }
    }

    // 5. è¯­éŸ³è´¨é‡ç»¼åˆè¯„ä¼°
    speechQualityHistory.current.push(speechConfidence);
    if (speechQualityHistory.current.length > 5) {
      speechQualityHistory.current.shift();
    }

    const avgSpeechQuality = speechQualityHistory.current.reduce((a, b) => a + b, 0) / speechQualityHistory.current.length;
    
    // æœ€ç»ˆåˆ¤æ–­
    const finalCheck = volume >= smartNoiseConfig.volumeThreshold && 
                      speechConfidence >= 60 && 
                      avgSpeechQuality >= 50;

    if (!finalCheck) {
      return { 
        isRealSpeech: false, 
        reason: `ç»¼åˆè¯„ä¼°ä¸é€šè¿‡: éŸ³é‡${volume.toFixed(1)}%ï¼Œç½®ä¿¡åº¦${speechConfidence.toFixed(1)}%ï¼Œå¹³å‡è´¨é‡${avgSpeechQuality.toFixed(1)}%`, 
        confidence: Math.min(confidence, 40) 
      };
    }

    confidence += 20;
    reasons.push('ç»¼åˆè¯„ä¼°é€šè¿‡');

    return { 
      isRealSpeech: true, 
      reason: reasons.join(' + '), 
      confidence: Math.min(confidence, 100) 
    };
  }, [smartNoiseConfig, recordingState, isBackgroundCalibrated, calibrateBackgroundNoise]);

  // VADäº‹ä»¶å¤„ç† - å¢å¼ºç‰ˆï¼Œæ·»åŠ è¯¦ç»†æ—¥å¿—
  const handleSpeechStart = useCallback(() => {
    if (isPaused || recordingState === RecordingState.RECORDING) return;
    
    const analysis = analyzeAudioIntelligently(
      audioMetrics.volume,
      audioMetrics.frequency, 
      audioMetrics.speechConfidence,
      audioMetrics.noiseLevel
    );
    
    setIsSpeechDetected(true);
    setLastSpeechTime(Date.now());
    
    console.log(`[VAD] è¯­éŸ³å¼€å§‹æ£€æµ‹ - ${analysis.isRealSpeech ? 'âœ… ç¡®è®¤' : 'âŒ æ‹’ç»'}: ${analysis.reason}`);
    
    if (!analysis.isRealSpeech) {
      console.log(`[æ™ºèƒ½é™å™ª] è¿‡æ»¤åŸå› : ${analysis.reason}`);
      return;
    }
    
    console.log('ğŸ¯ æ£€æµ‹åˆ°çœŸå®è¯­éŸ³ï¼Œå¼€å§‹å½•éŸ³ï¼');
    setSpeechStartTime(Date.now());
    startRecording();
  }, [isPaused, recordingState, audioMetrics, analyzeAudioIntelligently]);

  const handleSpeechEnd = useCallback(() => {
    setIsSpeechDetected(false);
    
    if (recordingState !== RecordingState.RECORDING) return;
    
    console.log('ğŸ”‡ æ£€æµ‹åˆ°è¯­éŸ³ç»“æŸï¼Œå¼€å§‹é™éŸ³è®¡æ—¶...');
    silenceTimerRef.current = setTimeout(() => {
      console.log('â° é™éŸ³è¶…æ—¶ï¼Œåœæ­¢å½•éŸ³');
      stopRecording();
    }, recordingConfig.silenceTimeout * 1000);
  }, [recordingState, recordingConfig.silenceTimeout]);

  const handleVADMisfire = useCallback(() => {
    console.log('âš ï¸ VADè¯¯è§¦å‘ - å·²è¢«æ™ºèƒ½é™å™ªè¿‡æ»¤');
  }, []);

  // åˆå§‹åŒ–éº¦å…‹é£å’ŒVAD
  const initializeMicrophone = async () => {
    try {
      updateRecordingState(RecordingState.IDLE);
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: recordingConfig.sampleRate,
          channelCount: 1,
          echoCancellation: recordingConfig.enableNoiseReduction,
          noiseSuppression: recordingConfig.enableNoiseReduction,
          autoGainControl: true,
        },
      });

      setMicStream(stream);
      await initializeVAD(stream);
      initializeAudioAnalysis(stream);
      updateRecordingState(RecordingState.LISTENING);
      setError(null);
      
    } catch (err) {
      console.error('éº¦å…‹é£åˆå§‹åŒ–å¤±è´¥:', err);
      setError('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®');
      updateRecordingState(RecordingState.ERROR);
    }
  };

  // åˆå§‹åŒ–VAD
  const initializeVAD = async (stream: MediaStream) => {
    try {
      if (vadRef.current) {
        vadRef.current.destroy();
      }

      const vad = await MicVAD.new({
        stream,
        positiveSpeechThreshold: vadConfig.positiveSpeechThreshold,
        negativeSpeechThreshold: vadConfig.negativeSpeechThreshold,
        minSpeechFrames: vadConfig.minSpeechFrames,
        preSpeechPadFrames: vadConfig.preSpeechPadFrames,
        redemptionFrames: vadConfig.redemptionFrames,
        onSpeechStart: handleSpeechStart,
        onSpeechEnd: handleSpeechEnd,
        onVADMisfire: handleVADMisfire,
      });

      vadRef.current = vad;
      setIsVadReady(true);
    } catch (err) {
      console.error('VADåˆå§‹åŒ–å¤±è´¥:', err);
      setError('è¯­éŸ³æ£€æµ‹ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥');
    }
  };

  // åˆå§‹åŒ–éŸ³é¢‘åˆ†æ
  const initializeAudioAnalysis = (stream: MediaStream) => {
    try {
      const audioContext = new AudioContext({ sampleRate: recordingConfig.sampleRate });
      const source = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      
      analyser.fftSize = 2048;
      analyser.minDecibels = -90;
      analyser.maxDecibels = -10;
      
      source.connect(analyser);
      
      audioContextRef.current = audioContext;
      analyserRef.current = analyser;
      
      startAudioAnalysis();
    } catch (err) {
      console.error('éŸ³é¢‘åˆ†æåˆå§‹åŒ–å¤±è´¥:', err);
    }
  };

  // å¼€å§‹éŸ³é¢‘åˆ†æ - å¢å¼ºç‰ˆ
  const startAudioAnalysis = () => {
    const analyser = analyserRef.current;
    if (!analyser) return;

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const timeDataArray = new Uint8Array(bufferLength);

    const analyze = () => {
      analyser.getByteFrequencyData(dataArray);
      analyser.getByteTimeDomainData(timeDataArray);
      
      // è®¡ç®—éŸ³é‡ï¼ˆRMSï¼‰
      let sum = 0;
      for (let i = 0; i < timeDataArray.length; i++) {
        const amplitude = (timeDataArray[i] - 128) / 128;
        sum += amplitude * amplitude;
      }
      const volume = Math.sqrt(sum / timeDataArray.length) * 100;

      // è®¡ç®—ä¸»é¢‘ç‡ï¼ˆäººå£°é¢‘ç‡èŒƒå›´ 300-3400Hzï¼‰
      const sampleRate = audioContextRef.current?.sampleRate || 16000;
      const frequencyPerBin = sampleRate / (2 * bufferLength);
      
      let maxValue = 0;
      let maxIndex = 0;
      const humanVoiceStart = Math.floor(300 / frequencyPerBin);
      const humanVoiceEnd = Math.floor(3400 / frequencyPerBin);
      
      for (let i = humanVoiceStart; i < humanVoiceEnd && i < bufferLength; i++) {
        if (dataArray[i] > maxValue) {
          maxValue = dataArray[i];
          maxIndex = i;
        }
      }
      const frequency = maxIndex * frequencyPerBin;

      // è®¡ç®—å™ªéŸ³æ°´å¹³ï¼ˆä½é¢‘éƒ¨åˆ†ï¼‰
      let noiseSum = 0;
      const noiseEnd = Math.floor(200 / frequencyPerBin);
      for (let i = 0; i < noiseEnd && i < bufferLength; i++) {
        noiseSum += dataArray[i];
      }
      const noiseLevel = (noiseSum / noiseEnd) / 255 * 100;

      // è®¡ç®—è¯­éŸ³ç½®ä¿¡åº¦ï¼ˆäººå£°é¢‘ç‡èŒƒå›´å†…çš„èƒ½é‡æ¯”ä¾‹ï¼‰
      let speechSum = 0;
      let totalSum = 0;
      for (let i = 0; i < bufferLength; i++) {
        totalSum += dataArray[i];
        if (i >= humanVoiceStart && i < humanVoiceEnd) {
          speechSum += dataArray[i];
        }
      }
      const speechConfidence = totalSum > 0 ? (speechSum / totalSum) * 100 : 0;

      setAudioMetrics({
        volume,
        frequency,
        speechConfidence,
        noiseLevel,
      });

      animationFrameRef.current = requestAnimationFrame(analyze);
    };

    analyze();
  };

  // å¼€å§‹å½•éŸ³
  const startRecording = () => {
    if (!micStream || recordingState === RecordingState.RECORDING) return;

    try {
      recordingChunks.current = [];
      
      const mediaRecorder = new MediaRecorder(micStream, {
        mimeType: 'audio/webm;codecs=opus',
      });

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          recordingChunks.current.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        processRecording();
      };

      mediaRecorder.start(100); // æ¯100msä¸€ä¸ªæ•°æ®å—
      mediaRecorderRef.current = mediaRecorder;
      
      setRecordingStartTime(Date.now());
      updateRecordingState(RecordingState.RECORDING);

      // è®¾ç½®æœ€å¤§å½•éŸ³æ—¶é•¿ä¿æŠ¤
      maxRecordingTimerRef.current = setTimeout(() => {
        stopRecording();
      }, recordingConfig.maxRecordingTime * 1000);

    } catch (err) {
      console.error('å½•éŸ³å¯åŠ¨å¤±è´¥:', err);
      setError('å½•éŸ³å¯åŠ¨å¤±è´¥');
    }
  };

  // åœæ­¢å½•éŸ³
  const stopRecording = () => {
    if (recordingState !== RecordingState.RECORDING) return;

    // æ¸…ç†å®šæ—¶å™¨
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current);
      silenceTimerRef.current = null;
    }
    if (maxRecordingTimerRef.current) {
      clearTimeout(maxRecordingTimerRef.current);
      maxRecordingTimerRef.current = null;
    }

    // åœæ­¢å½•éŸ³
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop();
    }

    updateRecordingState(RecordingState.PROCESSING);
  };

  // å¤„ç†å½•éŸ³ç»“æœ
  const processRecording = async () => {
    try {
      if (recordingChunks.current.length === 0) {
        updateRecordingState(RecordingState.LISTENING);
        return;
      }

      const audioBlob = new Blob(recordingChunks.current, { 
        type: 'audio/webm;codecs=opus' 
      });

      // ä¼ é€’å½“å‰çš„éŸ³é¢‘æŒ‡æ ‡
      onAudioReady(audioBlob, { ...audioMetrics });
      
      updateRecordingState(RecordingState.LISTENING);
    } catch (err) {
      console.error('å½•éŸ³å¤„ç†å¤±è´¥:', err);
      setError('å½•éŸ³å¤„ç†å¤±è´¥');
      updateRecordingState(RecordingState.ERROR);
    }
  };

  // æ‰‹åŠ¨å¼€å§‹ç›‘å¬
  const startListening = () => {
    if (!isVadReady) {
      initializeMicrophone();
    } else {
      vadRef.current?.start();
      updateRecordingState(RecordingState.LISTENING);
      // é‡ç½®èƒŒæ™¯å™ªéŸ³æ ¡å‡†
      setIsBackgroundCalibrated(false);
      backgroundNoiseSamples.current = [];
    }
  };

  // æ‰‹åŠ¨åœæ­¢ç›‘å¬
  const stopListening = () => {
    vadRef.current?.pause();
    if (recordingState === RecordingState.RECORDING) {
      stopRecording();
    }
    updateRecordingState(RecordingState.IDLE);
  };

  // æ›´æ–°VADé…ç½®
  const updateVADConfig = async (newConfig: Partial<VADConfig>) => {
    const updatedConfig = { ...vadConfig, ...newConfig };
    setVadConfig(updatedConfig);
    
    if (micStream && isVadReady) {
      await initializeVAD(micStream);
    }
  };

  // æ›´æ–°æ™ºèƒ½é™å™ªé…ç½® - æ–°å¢
  const updateSmartNoiseConfig = (newConfig: Partial<SmartNoiseConfig>) => {
    setSmartNoiseConfig(prev => ({ ...prev, ...newConfig }));
  };

  // æ‰‹åŠ¨æ ¡å‡†èƒŒæ™¯å™ªéŸ³ - æ–°å¢
  const manualCalibrateBackground = () => {
    setIsBackgroundCalibrated(false);
    backgroundNoiseSamples.current = [];
    setTimeout(() => {
      if (backgroundNoiseSamples.current.length > 10) {
        calibrateBackgroundNoise();
      }
    }, 3000); // 3ç§’åè‡ªåŠ¨æ ¡å‡†
  };

  // ç»„ä»¶å¸è½½æ¸…ç†
  useEffect(() => {
    return () => {
      vadRef.current?.destroy();
      micStream?.getTracks().forEach(track => track.stop());
      audioContextRef.current?.close();
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current);
      }
      if (maxRecordingTimerRef.current) {
        clearTimeout(maxRecordingTimerRef.current);
      }
    };
  }, []);

  // çŠ¶æ€æŒ‡ç¤ºå™¨é¢œè‰²
  const getStateColor = () => {
    switch (recordingState) {
      case RecordingState.RECORDING: return 'bg-red-500';
      case RecordingState.LISTENING: return 'bg-green-500';
      case RecordingState.PROCESSING: return 'bg-yellow-500';
      case RecordingState.ERROR: return 'bg-red-600';
      default: return 'bg-gray-500';
    }
  };

  // çŠ¶æ€æ–‡æœ¬
  const getStateText = () => {
    switch (recordingState) {
      case RecordingState.IDLE: return 'å¾…æœº';
      case RecordingState.LISTENING: return 'ç›‘å¬ä¸­';
      case RecordingState.RECORDING: return 'å½•éŸ³ä¸­';
      case RecordingState.PROCESSING: return 'å¤„ç†ä¸­';
      case RecordingState.ERROR: return 'é”™è¯¯';
    }
  };

  return (
    <div className={cn("space-y-4", className)}>
      {/* ä¸»æ§åˆ¶é¢æ¿ */}
      <Card>
        <CardHeader>
          <CardTitle className="flex items-center gap-2">
            <Activity className="h-5 w-5" />
            æ™ºèƒ½è¯­éŸ³å½•éŸ³å™¨
            {isBackgroundCalibrated && (
              <Badge variant="secondary" className="text-xs">
                <Shield className="w-3 h-3 mr-1" />
                é™å™ªå·²æ ¡å‡†
              </Badge>
            )}
          </CardTitle>
        </CardHeader>
        <CardContent className="space-y-4">
          {/* çŠ¶æ€æ˜¾ç¤º */}
          <div className="flex items-center justify-between p-4 rounded-lg bg-secondary">
            <div className="flex items-center gap-3">
              <div className={cn(
                "w-3 h-3 rounded-full transition-all duration-300",
                getStateColor(),
                recordingState === RecordingState.RECORDING && "animate-pulse"
              )} />
              <span className="font-medium">{getStateText()}</span>
              {/* æ–°å¢ï¼šè¯­éŸ³æ£€æµ‹çŠ¶æ€ */}
              {recordingState === RecordingState.LISTENING && (
                <Badge variant={isSpeechDetected ? "default" : "outline"} className="text-xs">
                  {isSpeechDetected ? "ğŸ¤ æ£€æµ‹åˆ°è¯­éŸ³" : "ğŸ”‡ ç­‰å¾…è¯­éŸ³"}
                </Badge>
              )}
            </div>
            <Badge variant={recordingState === RecordingState.RECORDING ? "destructive" : "default"}>
              {recordingState === RecordingState.RECORDING && "ğŸ”´ "}
              {isVadReady ? 'å°±ç»ª' : 'åŠ è½½ä¸­'}
            </Badge>
          </div>

          {/* è¯­éŸ³æ£€æµ‹è¯¦æƒ… - æ–°å¢ */}
          {recordingState === RecordingState.LISTENING && (
            <div className="p-3 rounded-lg bg-muted/50 space-y-2">
              <div className="flex justify-between items-center text-sm">
                <span className="font-medium">å®æ—¶æ£€æµ‹çŠ¶æ€</span>
                <span className={cn(
                  "font-mono",
                  isSpeechDetected ? "text-green-600" : "text-gray-500"
                )}>
                  {isSpeechDetected ? "è¯­éŸ³ä¸­" : "é™éŸ³"}
                </span>
              </div>
              
              <div className="grid grid-cols-2 gap-2 text-xs">
                <div className="flex justify-between">
                  <span>éŸ³é‡é˜ˆå€¼:</span>
                  <span className={audioMetrics.volume >= smartNoiseConfig.volumeThreshold ? "text-green-600" : "text-red-500"}>
                    {audioMetrics.volume.toFixed(1)}% / {smartNoiseConfig.volumeThreshold}%
                  </span>
                </div>
                <div className="flex justify-between">
                  <span>äººå£°é¢‘ç‡:</span>
                  <span className={audioMetrics.frequency >= 800 && audioMetrics.frequency <= 2000 ? "text-green-600" : "text-yellow-600"}>
                    {audioMetrics.frequency.toFixed(0)}Hz
                  </span>
                </div>
                <div className="flex justify-between">
                  <span>è¯­éŸ³ç½®ä¿¡:</span>
                  <span className={audioMetrics.speechConfidence >= 60 ? "text-green-600" : "text-red-500"}>
                    {audioMetrics.speechConfidence.toFixed(0)}%
                  </span>
                </div>
                <div className="flex justify-between">
                  <span>å™ªéŸ³æ°´å¹³:</span>
                  <span className={isBackgroundCalibrated && audioMetrics.noiseLevel <= smartNoiseConfig.backgroundNoiseLevel * 0.8 ? "text-green-600" : "text-yellow-600"}>
                    {audioMetrics.noiseLevel.toFixed(1)}%
                  </span>
                </div>
              </div>
              
              {lastSpeechTime > 0 && (
                <div className="text-xs text-muted-foreground border-t pt-2">
                  ä¸Šæ¬¡æ£€æµ‹: {new Date(lastSpeechTime).toLocaleTimeString()}
                </div>
              )}
            </div>
          )}

          {/* éŸ³é¢‘æŒ‡æ ‡ */}
          <div className="grid grid-cols-2 gap-4">
            <div>
              <div className="flex justify-between text-sm mb-1">
                <span>éŸ³é‡</span>
                <span>{audioMetrics.volume.toFixed(1)}%</span>
              </div>
              <Progress value={audioMetrics.volume} className="h-2" />
              {/* éŸ³é‡é˜ˆå€¼æŒ‡ç¤ºçº¿ */}
              <div className="relative mt-1">
                <div 
                  className="absolute h-0.5 bg-red-500 opacity-60"
                  style={{ 
                    left: `${smartNoiseConfig.volumeThreshold}%`,
                    width: '2px',
                    top: '-6px'
                  }}
                />
                <div className="text-xs text-red-500" style={{ marginLeft: `${smartNoiseConfig.volumeThreshold}%` }}>
                  é˜ˆå€¼
                </div>
              </div>
            </div>
            <div>
              <div className="flex justify-between text-sm mb-1">
                <span>è¯­éŸ³ç½®ä¿¡åº¦</span>
                <span>{audioMetrics.speechConfidence.toFixed(1)}%</span>
              </div>
              <Progress value={audioMetrics.speechConfidence} className="h-2" />
              {/* ç½®ä¿¡åº¦è¯´æ˜ */}
              <div className="text-xs text-muted-foreground mt-1">
                {audioMetrics.speechConfidence >= 80 ? "é«˜ç½®ä¿¡åº¦ - å¯èƒ½æ˜¯äººå£°" :
                 audioMetrics.speechConfidence >= 60 ? "ä¸­ç­‰ç½®ä¿¡åº¦ - éœ€è¦å…¶ä»–æ¡ä»¶éªŒè¯" :
                 audioMetrics.speechConfidence >= 40 ? "ä½ç½®ä¿¡åº¦ - å¯èƒ½ä¸æ˜¯äººå£°" :
                 "æä½ç½®ä¿¡åº¦ - å¤§æ¦‚ç‡ä¸æ˜¯äººå£°"}
              </div>
            </div>
          </div>

          {/* æ§åˆ¶æŒ‰é’® */}
          <div className="flex gap-2">
            {recordingState === RecordingState.IDLE ? (
              <Button 
                onClick={startListening} 
                disabled={isLoading}
                className="flex-1"
              >
                <Mic className="w-4 h-4 mr-2" />
                å¼€å§‹ç›‘å¬
              </Button>
            ) : (
              <Button 
                onClick={stopListening}
                variant="destructive"
                className="flex-1"
              >
                <Square className="w-4 h-4 mr-2" />
                åœæ­¢ç›‘å¬
              </Button>
            )}
            
            {/* èƒŒæ™¯å™ªéŸ³æ ¡å‡†æŒ‰é’® */}
            <Button 
              onClick={manualCalibrateBackground}
              variant="outline"
              size="sm"
              disabled={recordingState === RecordingState.IDLE}
            >
              <Shield className="w-4 h-4 mr-1" />
              æ ¡å‡†èƒŒæ™¯éŸ³
            </Button>
          </div>

          {error && (
            <div className="text-sm text-red-600 bg-red-50 p-2 rounded">
              {error}
            </div>
          )}
        </CardContent>
      </Card>

      {/* æ™ºèƒ½é™å™ªæ§åˆ¶é¢æ¿ */}
      {showAdvancedControls && (
        <Card>
          <CardHeader>
            <CardTitle className="flex items-center gap-2">
              <Shield className="h-5 w-5" />
              æ™ºèƒ½é™å™ªè®¾ç½®
            </CardTitle>
          </CardHeader>
          <CardContent className="space-y-4">
            <div className="grid md:grid-cols-2 gap-4">
              <div>
                <label className="text-sm font-medium">éŸ³é‡é—¨æ§é˜ˆå€¼</label>
                <Slider
                  value={[smartNoiseConfig.volumeThreshold]}
                  onValueChange={([value]) => updateSmartNoiseConfig({ volumeThreshold: value })}
                  max={50}
                  min={5}
                  step={1}
                  className="mt-2"
                />
                <div className="text-xs text-right mt-1">{smartNoiseConfig.volumeThreshold}%</div>
                <p className="text-xs text-muted-foreground mt-1">
                  ä½äºæ­¤éŸ³é‡çš„å£°éŸ³å°†è¢«å¿½ç•¥
                </p>
              </div>
              
              <div>
                <label className="text-sm font-medium">èƒŒæ™¯å™ªéŸ³åŸºå‡†</label>
                <div className="mt-2 p-2 bg-secondary rounded text-sm">
                  {isBackgroundCalibrated ? 
                    `${smartNoiseConfig.backgroundNoiseLevel.toFixed(1)}%` : 
                    'æœªæ ¡å‡†'
                  }
                </div>
                <p className="text-xs text-muted-foreground mt-1">
                  è‡ªåŠ¨æ£€æµ‹çš„èƒŒæ™¯å™ªéŸ³æ°´å¹³
                </p>
              </div>
            </div>

            <Separator />

            <div className="grid grid-cols-2 gap-4 text-sm">
              <div className="space-y-2">
                <div className="flex items-center justify-between">
                  <span>éŸ³é‡é—¨æ§</span>
                  <input 
                    type="checkbox" 
                    checked={smartNoiseConfig.enableVolumeGating}
                    onChange={(e) => updateSmartNoiseConfig({ enableVolumeGating: e.target.checked })}
                  />
                </div>
                <div className="flex items-center justify-between">
                  <span>é¢‘ç‡æ»¤æ³¢</span>
                  <input 
                    type="checkbox" 
                    checked={smartNoiseConfig.enableFrequencyFiltering}
                    onChange={(e) => updateSmartNoiseConfig({ enableFrequencyFiltering: e.target.checked })}
                  />
                </div>
              </div>
              <div className="space-y-2">
                <div className="flex items-center justify-between">
                  <span>æ–¹å‘æ€§æ£€æµ‹</span>
                  <input 
                    type="checkbox" 
                    checked={smartNoiseConfig.enableDirectionalDetection}
                    onChange={(e) => updateSmartNoiseConfig({ enableDirectionalDetection: e.target.checked })}
                  />
                </div>
                <div className="flex items-center justify-between">
                  <span>è‡ªé€‚åº”é˜ˆå€¼</span>
                  <input 
                    type="checkbox" 
                    checked={smartNoiseConfig.enableAdaptiveThreshold}
                    onChange={(e) => updateSmartNoiseConfig({ enableAdaptiveThreshold: e.target.checked })}
                  />
                </div>
              </div>
            </div>

            <div className="pt-2 border-t text-xs text-muted-foreground">
              <div className="grid grid-cols-2 gap-4">
                <div>
                  <strong>å½“å‰é¢‘ç‡:</strong> {audioMetrics.frequency.toFixed(0)}Hz
                </div>
                <div>
                  <strong>å™ªéŸ³æ°´å¹³:</strong> {audioMetrics.noiseLevel.toFixed(1)}%
                </div>
              </div>
            </div>
          </CardContent>
        </Card>
      )}

      {/* é«˜çº§æ§åˆ¶é¢æ¿ */}
      {showAdvancedControls && (
        <Card>
          <CardHeader>
            <CardTitle className="flex items-center gap-2">
              <Settings className="h-5 w-5" />
              é«˜çº§è®¾ç½®
            </CardTitle>
          </CardHeader>
          <CardContent className="space-y-4">
            <div className="grid md:grid-cols-2 gap-4">
              <div>
                <label className="text-sm font-medium">è¯­éŸ³æ£€æµ‹é˜ˆå€¼</label>
                <Slider
                  value={[vadConfig.positiveSpeechThreshold]}
                  onValueChange={([value]) => updateVADConfig({ positiveSpeechThreshold: value })}
                  max={1}
                  min={0}
                  step={0.05}
                  className="mt-2"
                />
                <div className="text-xs text-right mt-1">{vadConfig.positiveSpeechThreshold}</div>
              </div>
              
              <div>
                <label className="text-sm font-medium">é™éŸ³è¶…æ—¶ (ç§’)</label>
                <Slider
                  value={[recordingConfig.silenceTimeout]}
                  onValueChange={([value]) => setRecordingConfig(prev => ({ ...prev, silenceTimeout: value }))}
                  max={5}
                  min={0.5}
                  step={0.5}
                  className="mt-2"
                />
                <div className="text-xs text-right mt-1">{recordingConfig.silenceTimeout}s</div>
              </div>
            </div>

            <Separator />

            <div className="grid grid-cols-3 gap-4 text-sm">
              <div className="text-center">
                <div className="text-lg font-bold text-blue-600">
                  {audioMetrics.frequency.toFixed(0)}Hz
                </div>
                <div className="text-muted-foreground">ä¸»é¢‘ç‡</div>
              </div>
              <div className="text-center">
                <div className="text-lg font-bold text-purple-600">
                  {audioMetrics.noiseLevel.toFixed(1)}%
                </div>
                <div className="text-muted-foreground">å™ªéŸ³æ°´å¹³</div>
              </div>
              <div className="text-center">
                <div className="text-lg font-bold text-green-600">
                  {recordingStartTime > 0 ? ((Date.now() - recordingStartTime) / 1000).toFixed(1) : '0'}s
                </div>
                <div className="text-muted-foreground">å½•éŸ³æ—¶é•¿</div>
              </div>
            </div>
          </CardContent>
        </Card>
      )}
    </div>
  );
};

export default SmartVoiceRecorder; 